{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lessgo?.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvZDlGPpvqXv"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import numpy as np\n",
        "import os, csv\n",
        "from PIL import Image\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f9LwYJLMzhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f251d45e-0bb3-4821-c915-dac238c8dca5"
      },
      "source": [
        "#RUN THIS CELL TO IMPORT G_DRIVE\n",
        "#Share dataset to your drive first.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnmTpcOMt8xZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ed8a80-8c95-4cdb-a3f7-a2c6c220cf16"
      },
      "source": [
        "os.chdir(r'/content/drive/MyDrive/ES_FaceMatch_Dataset')\n",
        "print(os.getcwd())\n",
        "\n",
        "train = csv.reader(open('train.csv'))\n",
        "train_data = []\n",
        "\n",
        "for line in train:\n",
        "    try:\n",
        "        train_data.append([line[0], line[1], int(line[2])])\n",
        "    except:\n",
        "        pass\n",
        "print(train_data[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1jVtyGeVkD0wuaIwqQ0-8rkJz8KDKIeOk/ES_FaceMatch_Dataset\n",
            "['19865480214299.jpg', '55378374591770.jpg', 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9py3rjLzS0-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efc8d98f-3713-43d3-fd18-482cad81e406"
      },
      "source": [
        "img = np.asarray(Image.open(r'dataset_images/'+train_data[2][0]))\n",
        "print(img.shape, type(img))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 256, 3) <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KNgmXvdZH8l"
      },
      "source": [
        "train_input1 = []\n",
        "train_input2 = []\n",
        "train_output = []\n",
        "count = 0\n",
        "img1 = None\n",
        "img2 = None\n",
        "for i in train_data:\n",
        "    img1 = cv2.imread(r'dataset_images/'+i[0],cv2.IMREAD_COLOR)\n",
        "    img1 = cv2.resize(img1,(256,256))\n",
        "    #train_input1.append(np.asarray(Image.open(r'dataset_images/'+i[0])))\n",
        "    train_input1.append(img1)\n",
        "    \n",
        "    img2 = cv2.imread(r'dataset_images/'+i[1],cv2.IMREAD_COLOR)\n",
        "    img2 = cv2.resize(img2,(256,256))\n",
        "    #train_input2.append(np.asarray(Image.open(r'dataset_images/'+i[1])))\n",
        "    train_input2.append(img2)\n",
        "    train_output.append(i[2])\n",
        "    count += 1\n",
        "    if count >= 5000:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1VxB3j4V6Fo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d008ecde-3e23-47bd-ae2e-6c872a9179c7"
      },
      "source": [
        "train_input1_model = np.array(train_input1[0:3500])\n",
        "train_input2_model = np.array(train_input2[0:3500])\n",
        "output_train = np.array(train_output[0:3500])\n",
        "val_input1_model = np.array(train_input1[3500:5000])\n",
        "val_input2_model = np.array(train_input2[3500:5000])\n",
        "output_val = np.array(train_output[3500:5000])\n",
        "print(len(train_input1_model))\n",
        "print(len(output_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500\n",
            "1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX-jJA1FuEBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56e0c7d-a260-4620-a801-b508e57cf3fc"
      },
      "source": [
        "input1 = keras.layers.Input((256,256,3,)) #change 256 to the image dimension obtained after preprocessing. Should we do grayscale\n",
        "input2 = keras.layers.Input((256,256,3,))\n",
        "input = concatenate([input1, input2],axis=-1)\n",
        "input.shape\n",
        "\n",
        "print(train_input1_model[0].shape, len(train_input2_model), len(output_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 256, 3) 3500 3500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPMuYAFNu69B"
      },
      "source": [
        "conv1 =  tf.keras.layers.Conv2D(filters=64, kernel_size=(7,7), strides=(2,2), padding='same',activation='relu', activity_regularizer=regularizers.l2(0.01))(input) #added some convolutions and dense layers hoping to get good representations\n",
        "pool1 =  tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(conv1)                                         \n",
        "bn =  tf.keras.layers.BatchNormalization()(pool1)                                                                                          \n",
        "conv2 =  tf.keras.layers.Conv2D(filters=64, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu', activity_regularizer=regularizers.l2(0.01))(bn)  \n",
        "conv3 =  tf.keras.layers.Conv2D(filters=192, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu', activity_regularizer=regularizers.l2(0.01))(conv2) \n",
        "bn2 =  tf.keras.layers.BatchNormalization()(conv3)                                                                    \n",
        "pool2 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(bn2)\n",
        "conv4 = tf.keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu', activity_regularizer=regularizers.l2(0.01))(pool2) # new\n",
        "conv5 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu', activity_regularizer=regularizers.l2(0.01))(conv4)\n",
        "conv6 = tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu', activity_regularizer=regularizers.l2(0.01))(conv5)\n",
        "conv7 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu', activity_regularizer=regularizers.l2(0.01)),\n",
        "pool3 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same', activity_regularizer=regularizers.l2(0.01))(conv6)                                    \n",
        "flat =  tf.keras.layers.Flatten()(pool3)\n",
        "dense1 =  tf.keras.layers.Dense(128,activation='relu', activity_regularizer=regularizers.l2(0.01))(flat)\n",
        "norm1 = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(dense1) # L2 normalize embeddings\n",
        "dense2 =  tf.keras.layers.Dense(128,activation='relu', activity_regularizer=regularizers.l2(0.01))(norm1)\n",
        "dense3 = tf.keras.layers.Dense(32,activation='relu' , activity_regularizer=regularizers.l2(0.01))(dense2)\n",
        "output = tf.keras.layers.Dense(1,activation = 'sigmoid', activity_regularizer=regularizers.l2(0.01))(dense3) #has to be 0 or 1\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvwkEoZR0JMm"
      },
      "source": [
        "model = Model([input1, input2], output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6lrKEvg0S73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b5560d-be8b-4b75-c0e0-b49550f36117"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 256, 256, 6)  0           input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 128, 128, 64) 18880       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 64, 64, 64)   4160        batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 64, 64, 192)  110784      conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 64, 64, 192)  768         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 192)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 384)  74112       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 256)  884992      conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 256)  65792       conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 65536)        0           max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          8388736     flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 128)          0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          16512       lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 32)           4128        dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            33          dense_10[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 9,569,153\n",
            "Trainable params: 9,568,641\n",
            "Non-trainable params: 512\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DdwVD5C1K7h"
      },
      "source": [
        "#train the model here\n",
        "model.compile(optimizer=Adam(learning_rate=1e-3), loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7hPBCOBX5I_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "652ff5b2-7ec4-4136-fb12-58bd20147918"
      },
      "source": [
        "model.fit(\n",
        "    x = [train_input1_model, train_input2_model],\n",
        "    #x = train_input1_model,\n",
        "    y = output_train,\n",
        "    batch_size = 200,\n",
        "    epochs = 10,\n",
        "    verbose = 1,\n",
        "    validation_data = ([val_input1_model, val_input2_model], output_val),\n",
        "    #validation_data = train_input2_model,\n",
        "    shuffle = True, # doesn't matter if we have only one epoch or no batches,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "18/18 [==============================] - 1086s 60s/step - loss: 1686069.2500 - binary_accuracy: 0.6174 - val_loss: 104.9500 - val_binary_accuracy: 0.6260\n",
            "Epoch 2/10\n",
            "18/18 [==============================] - 1080s 60s/step - loss: 106.4931 - binary_accuracy: 0.6234 - val_loss: 86.2394 - val_binary_accuracy: 0.6260\n",
            "Epoch 3/10\n"
          ]
        }
      ]
    }
  ]
}